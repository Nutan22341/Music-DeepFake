{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10402084,"sourceType":"datasetVersion","datasetId":6445498},{"sourceId":10982178,"sourceType":"datasetVersion","datasetId":6834683}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T20:11:15.736418Z","iopub.execute_input":"2025-03-20T20:11:15.736634Z","iopub.status.idle":"2025-03-20T20:11:15.761281Z","shell.execute_reply.started":"2025-03-20T20:11:15.736613Z","shell.execute_reply":"2025-03-20T20:11:15.759450Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/deepfakedataset'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T20:20:37.867104Z","iopub.execute_input":"2025-01-27T20:20:37.867517Z","iopub.status.idle":"2025-01-27T20:20:37.872494Z","shell.execute_reply.started":"2025-01-27T20:20:37.867484Z","shell.execute_reply":"2025-01-27T20:20:37.871190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for root, dirs, files in os.walk(BASE_DIR):\n    print(f\"Root: {root}, Directories: {dirs}, Files: {len(files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T14:10:28.757090Z","iopub.execute_input":"2025-01-09T14:10:28.757491Z","iopub.status.idle":"2025-01-09T14:11:41.539588Z","shell.execute_reply.started":"2025-01-09T14:10:28.757458Z","shell.execute_reply":"2025-01-09T14:11:41.537968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\n# Initialize variables to track maximum and minimum durations\nmax_duration = -np.inf\nmin_duration = np.inf\nmax_duration_file = None\nmin_duration_file = None\n\n# Function to analyze individual audio file\ndef analyze_audio(file_path):\n    global max_duration, min_duration, max_duration_file, min_duration_file\n\n    # Load audio file\n    audio, sr = librosa.load(file_path, sr=None)\n    \n    # Compute the duration\n    duration = librosa.get_duration(y=audio, sr=sr)\n    \n    # Update maximum and minimum durations\n    if duration > max_duration:\n        max_duration = duration\n        max_duration_file = file_path\n    if duration < min_duration:\n        min_duration = duration\n        min_duration_file = file_path\n\n    # Print basic audio properties\n    print(f\"Audio File: {file_path}\")\n    print(f\"Sample Rate: {sr}\")\n    print(f\"Duration: {duration:.2f} seconds\")\n    \n    # Plot the waveform\n    plt.figure(figsize=(10, 4))\n    librosa.display.waveshow(audio, sr=sr)\n    plt.title('Waveform')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude')\n    plt.show()\n\n    # Compute and plot the spectrogram\n    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)  # Fixed typo\n    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n    \n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(spectrogram_db, sr=sr, x_axis='time', y_axis='mel', cmap='coolwarm')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Mel-Spectrogram')\n    plt.show()\n\n# Loop through all files in the directory\nfor root, dirs, files in os.walk(BASE_DIR):\n    for file in files:\n        if file.endswith(('.wav', '.mp3', '.flac')):  # Check for valid audio file extensions\n            file_path = os.path.join(root, file)  # Construct the full path\n            analyze_audio(file_path)  # Pass the full path to the function\n\n# Print the results for maximum and minimum durations\nprint(\"\\nAnalysis Complete:\")\nprint(f\"Longest Audio: {max_duration_file} ({max_duration:.2f} seconds)\")\nprint(f\"Shortest Audio: {min_duration_file} ({min_duration:.2f} seconds)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T20:20:47.060894Z","iopub.execute_input":"2025-01-27T20:20:47.061300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### import os\nimport csv\n\n# Define the root directory (adjust this based on your Kaggle dataset path)\nroot_dir = \"/kaggle/input/deepfakedataset/FakeMusicCaps\"  \noutput_csv = \"fake musiccaps_metadata.csv\"\n\n# Initialize a list to store file paths and labels\ndata = []\n\n# Traverse through each subfolder in the root directory\nfor folder_name in os.listdir(root_dir):\n    folder_path = os.path.join(root_dir, folder_name)\n    \n    # Skip non-folder entries\n    if not os.path.isdir(folder_path):\n        continue\n    \n    # Iterate through each file in the folder\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        \n        # Check if the file is an audio file (adjust extensions if necessary)\n        if file_name.endswith(('.mp3', '.wav', '.flac', '.aac')):\n            data.append([file_path, folder_name])  # Add file path and folder label to data\n\n# Write the data to a CSV file\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['file_path', 'label'])  # Header\n    writer.writerows(data)\n\nprint(f\"Metadata CSV file has been saved as {output_csv}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nfile_path = './fakemusiccaps_metadata.csv'  \ndf = pd.read_csv(file_path)\n\n# Display basic information\nprint(\"Dataset Overview\")\nprint(df.info())\nprint(\"\\nFirst Five Rows\")\nprint(df.head())\n\n# Checking for missing values\nprint(\"\\nMissing Values:\")\nprint(df.isnull().sum())\n\n# Descriptive statistics\nprint(\"\\nDescriptive Statistics:\")\nprint(df.describe())\n\n# Exploring unique classes in the dataset\nif 'label' in df.columns:  # Replace 'label' with the column name for class labels\n    print(\"\\nClass Distribution:\")\n    print(df['label'].value_counts())\n\n    # Visualizing class distribution\n    plt.figure(figsize=(10, 6))\n    sns.countplot(data=df, x='label', order=df['label'].value_counts().index, palette='viridis')\n    plt.title(\"Class Distribution\")\n    plt.xlabel(\"Labels\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    plt.show()\n\n# Exploring durations if available\nif 'duration' in df.columns:  # Replace 'duration' with the actual column name\n    print(\"\\nDuration Statistics:\")\n    print(df['duration'].describe())\n\n    # Visualizing durations\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df['duration'], bins=30, kde=True, color='blue')\n    plt.title(\"Distribution of Durations\")\n    plt.xlabel(\"Duration\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\n# Checking for duplicates\nduplicates = df.duplicated().sum()\nprint(f\"\\nNumber of duplicate rows: {duplicates}\")\n\n# Correlation analysis if numeric columns are available\nif df.select_dtypes(include=['float64', 'int64']).shape[1] > 1:\n    print(\"\\nCorrelation Matrix:\")\n    correlation_matrix = df.corr()\n    print(correlation_matrix)\n\n    # Heatmap visualization\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title(\"Correlation Heatmap\")\n    plt.show()\n\n# Any additional column-specific analysis\nprint(\"\\nUnique Columns:\")\nfor col in df.columns:\n    unique_values = df[col].nunique()\n    print(f\"{col}: {unique_values} unique values\")\n\n    if unique_values < 15:  # Show value counts for categorical columns with fewer unique values\n        print(df[col].value_counts())\n        print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:56:49.064544Z","iopub.execute_input":"2025-01-18T15:56:49.064950Z","iopub.status.idle":"2025-01-18T15:56:50.482038Z","shell.execute_reply.started":"2025-01-18T15:56:49.064919Z","shell.execute_reply":"2025-01-18T15:56:50.480889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CNN Architecture","metadata":{}},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate spectrogram\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=16000)  # Consistent sampling rate\n    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n    return spectrogram_db\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256):\n    spectrograms = []\n    fixed_labels = []\n    \n    for file_path, label in zip(file_paths, labels):\n        try:\n            spectrogram = extract_features(file_path)\n            if spectrogram.shape[1] > max_length:\n                spectrogram = spectrogram[:, :max_length]\n            else:\n                padding = max_length - spectrogram.shape[1]\n                spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n            \n            spectrogram /= (np.max(spectrogram) + 1e-8)  # Normalize after padding\n            spectrograms.append(spectrogram)\n            fixed_labels.append(label)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    return np.array(spectrograms), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Prepare dataset\nMAX_LENGTH = 256\nspectrograms, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH)\n\n# Add channel dimension\nspectrograms = spectrograms[..., np.newaxis]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    spectrograms, encoded_labels, test_size=0.2, random_state=42\n)\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Build CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(128, MAX_LENGTH, 1)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\n# Save the model\nmodel.save('audio_classification_cnn.h5')\n\n# Save the label mapping\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('label_mapping.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T03:09:27.617787Z","iopub.execute_input":"2025-01-17T03:09:27.618156Z","execution_failed":"2025-01-17T09:51:10.080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate spectrogram\ndef extract_features(file_path, sr=22050, n_fft=2048, hop_length=512, n_mels=128):\n    try:\n        audio, sr = librosa.load(file_path, sr=sr)\n        spectrogram = librosa.feature.melspectrogram(\n            y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n        )\n        spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n        return spectrogram_db\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return None\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256, sr=16000):\n    spectrograms, fixed_labels = [], []\n    for file_path, label in zip(file_paths, labels):\n        spectrogram = extract_features(file_path, sr=sr)\n        if spectrogram is not None:\n            # Truncate or pad to fixed length\n            if spectrogram.shape[1] > max_length:\n                spectrogram = spectrogram[:, :max_length]\n            else:\n                padding = max_length - spectrogram.shape[1]\n                spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n            spectrograms.append(spectrogram)\n            fixed_labels.append(label)\n    return np.array(spectrograms), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Define the fixed spectrogram length\nMAX_LENGTH = 256\n\n# Prepare dataset\nspectrograms, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH)\n\n# Normalize spectrograms and add channel dimension\nspectrograms = spectrograms / np.max(spectrograms)  # Normalize\nspectrograms = spectrograms[..., np.newaxis]  # Add channel dimension\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    spectrograms, encoded_labels, test_size=0.2, random_state=42\n)\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(encoded_labels),\n    y=encoded_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Build CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(128, MAX_LENGTH, 1)),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model with class weights\nhistory = model.fit(\n    X_train, y_train,\n    epochs=10, batch_size=32,\n    validation_split=0.2,\n    class_weight=class_weights,\n    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n)\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\n# Save the model\nmodel.save('audio_classification_cnn.h5')\n\n# Save the label mapping for decoding predictions\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('label_mapping.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:56:57.054294Z","iopub.execute_input":"2025-01-18T15:56:57.054821Z","iopub.status.idle":"2025-01-18T20:54:38.428954Z","shell.execute_reply.started":"2025-01-18T15:56:57.054786Z","shell.execute_reply":"2025-01-18T20:54:38.426490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate spectrogram\ndef extract_features(file_path, sr=22050, n_fft=2048, hop_length=512, n_mels=128):\n    try:\n        audio, sr = librosa.load(file_path, sr=sr)\n        spectrogram = librosa.feature.melspectrogram(\n            y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n        )\n        spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n        return spectrogram_db\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return None\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256, sr=16000):\n    spectrograms, fixed_labels = [], []\n    for file_path, label in zip(file_paths, labels):\n        spectrogram = extract_features(file_path, sr=sr)\n        if spectrogram is not None:\n            # Truncate or pad to fixed length\n            if spectrogram.shape[1] > max_length:\n                spectrogram = spectrogram[:, :max_length]\n            else:\n                padding = max_length - spectrogram.shape[1]\n                spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n            spectrograms.append(spectrogram)\n            fixed_labels.append(label)\n    return np.array(spectrograms), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Define the fixed spectrogram length\nMAX_LENGTH = 256\n\n# Prepare dataset\nspectrograms, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH)\n\n# Flatten spectrograms for ANN\nX_flat = spectrograms.reshape(spectrograms.shape[0], -1)  # Flatten each spectrogram\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_flat, encoded_labels, test_size=0.2, random_state=42\n)\n\n# Normalize flattened spectrograms\nX_train = X_train / np.max(X_train, axis=1, keepdims=True)  # Normalize each sample\nX_test = X_test / np.max(X_test, axis=1, keepdims=True)  # Normalize each sample\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(encoded_labels),\n    y=encoded_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Build ANN model\nann_model = Sequential([\n    Dense(512, activation='relu', input_shape=(X_flat.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')  # Output layer with softmax activation\n])\n\n# Compile the ANN model\nann_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the ANN model\nann_history = ann_model.fit(\n    X_train, y_train,\n    epochs=10, batch_size=32,\n    validation_split=0.2,\n    class_weight=class_weights,\n    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n)\n\n# Evaluate the ANN model\nann_test_loss, ann_test_acc = ann_model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy (ANN): {ann_test_acc:.2f}\")\n\n# Save the ANN model\nann_model.save('audio_classification_ann.keras')\n\n# Save the label mapping for decoding predictions\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('ann_label_mapping.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:46:54.149705Z","iopub.execute_input":"2025-01-20T09:46:54.150170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport csv\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the root directory (adjust this based on your Kaggle dataset path)\nroot_dir = \"/kaggle/input/deepfakedataset/FakeMusicCaps\"  # Adjust this path as needed\noutput_csv = \"fakemusiccaps_metadata1.csv\"\n\n# Initialize a list to store file paths and labels\ndata = []\n\n# Traverse through each subfolder in the root directory\nfor folder_name in os.listdir(root_dir):\n    folder_path = os.path.join(root_dir, folder_name)\n    \n    # Skip non-folder entries\n    if not os.path.isdir(folder_path):\n        continue\n    \n    # Iterate through each file in the folder\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        \n        # Check if the file is an audio file (adjust extensions if necessary)\n        if file_name.endswith(('.mp3', '.wav', '.flac', '.aac')):\n            data.append([file_path, folder_name])  # Add file path and folder label to data\n\n# Write the data to a CSV file\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['file_path', 'label'])  # Header\n    writer.writerows(data)\n\nprint(f\"Metadata CSV file has been saved as {output_csv}\")\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata1.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate MFCC features\ndef extract_mfcc(file_path, n_mfcc=13):\n    audio, sr = librosa.load(file_path, sr=16000)  # Consistent sampling rate\n    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    return mfcc\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256, n_mfcc=13):\n    mfcc_features = []\n    fixed_labels = []\n    \n    for file_path, label in zip(file_paths, labels):\n        try:\n            mfcc = extract_mfcc(file_path, n_mfcc=n_mfcc)\n            if mfcc.shape[1] > max_length:\n                mfcc = mfcc[:, :max_length]\n            else:\n                padding = max_length - mfcc.shape[1]\n                mfcc = np.pad(mfcc, ((0, 0), (0, padding)), mode='constant')\n            \n            mfcc /= (np.max(np.abs(mfcc)) + 1e-8)  # Normalize after padding\n            mfcc_features.append(mfcc)\n            fixed_labels.append(label)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    return np.array(mfcc_features), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Prepare dataset\nMAX_LENGTH = 256\nN_MFCC = 13\nmfcc_features, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH, n_mfcc=N_MFCC)\n\n# Add channel dimension\nmfcc_features = mfcc_features[..., np.newaxis]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    mfcc_features, encoded_labels, test_size=0.2, random_state=42\n)\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Build CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(N_MFCC, MAX_LENGTH, 1)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\n# Save the model\nmodel.save('audio_classification_cnn_mfcc.keras')\n\n# Save the label mapping\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('label_mapping.csv', index=False)\n\nprint(\"Model and label mapping saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:15:20.933209Z","iopub.execute_input":"2025-01-22T16:15:20.933588Z","iopub.status.idle":"2025-01-22T16:56:51.769714Z","shell.execute_reply.started":"2025-01-22T16:15:20.933560Z","shell.execute_reply":"2025-01-22T16:56:51.767328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport csv\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the root directory (adjust this based on your Kaggle dataset path)\nroot_dir = \"/kaggle/input/deepfakedataset/FakeMusicCaps\"  # Adjust this path as needed\noutput_csv = \"fakemusiccaps_metadata1.csv\"\n\n# Initialize a list to store file paths and labels\ndata = []\n\n# Traverse through each subfolder in the root directory\nfor folder_name in os.listdir(root_dir):\n    folder_path = os.path.join(root_dir, folder_name)\n    \n    # Skip non-folder entries\n    if not os.path.isdir(folder_path):\n        continue\n    \n    # Iterate through each file in the folder\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        \n        # Check if the file is an audio file (adjust extensions if necessary)\n        if file_name.endswith(('.mp3', '.wav', '.flac', '.aac')):\n            data.append([file_path, folder_name])  # Add file path and folder label to data\n\n# Write the data to a CSV file\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['file_path', 'label'])  # Header\n    writer.writerows(data)\n\nprint(f\"Metadata CSV file has been saved as {output_csv}\")\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata1.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate MFCC features\ndef extract_mfcc(file_path, n_mfcc=13):\n    audio, sr = librosa.load(file_path, sr=22050)  # Consistent sampling rate\n    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    return mfcc\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256, n_mfcc=13):\n    mfcc_features = []\n    fixed_labels = []\n    \n    for file_path, label in zip(file_paths, labels):\n        try:\n            mfcc = extract_mfcc(file_path, n_mfcc=n_mfcc)\n            if mfcc.shape[1] > max_length:\n                mfcc = mfcc[:, :max_length]\n            else:\n                padding = max_length - mfcc.shape[1]\n                mfcc = np.pad(mfcc, ((0, 0), (0, padding)), mode='constant')\n            \n            mfcc /= (np.max(np.abs(mfcc)) + 1e-8)  # Normalize after padding\n            mfcc_features.append(mfcc)\n            fixed_labels.append(label)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    return np.array(mfcc_features), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Prepare dataset\nMAX_LENGTH = 256\nN_MFCC = 13\nmfcc_features, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH, n_mfcc=N_MFCC)\n\n# Add channel dimension\nmfcc_features = mfcc_features[..., np.newaxis]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    mfcc_features, encoded_labels, test_size=0.2, random_state=42\n)\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Build CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(N_MFCC, MAX_LENGTH, 1)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\n# Save the model\nmodel.save('audio_classification_cnn_mfcc2.keras')\n\n# Save the label mapping\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('label_mapping.csv', index=False)\n\nprint(\"Model and label mapping saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:13:26.110391Z","iopub.execute_input":"2025-01-22T17:13:26.110879Z","iopub.status.idle":"2025-01-22T18:00:35.820234Z","shell.execute_reply.started":"2025-01-22T17:13:26.110848Z","shell.execute_reply":"2025-01-22T18:00:35.817671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate spectrogram\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=16000)  # Consistent sampling rate\n    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n    return spectrogram_db\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256):\n    spectrograms = []\n    fixed_labels = []\n    \n    for file_path, label in zip(file_paths, labels):\n        try:\n            spectrogram = extract_features(file_path)\n            if spectrogram.shape[1] > max_length:\n                spectrogram = spectrogram[:, :max_length]\n            else:\n                padding = max_length - spectrogram.shape[1]\n                spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n            \n            spectrogram /= (np.max(spectrogram) + 1e-8) \n            spectrograms.append(spectrogram)\n            fixed_labels.append(label)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    return np.array(spectrograms), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Prepare dataset\nMAX_LENGTH = 256\nspectrograms, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH)\n\n# Add channel dimension\nspectrograms = spectrograms[..., np.newaxis]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    spectrograms, encoded_labels, test_size=0.2, random_state=42\n)\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Build CNN model\nmodel_eer = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(128, MAX_LENGTH, 1)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')\n])\n\n# Compile model\nmodel_eer.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model_eer.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate model\ntest_loss, test_acc = model_eer.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\n# Save the model\nmodel_eer.save('audio_classification_cnn.h5')\n\n# Save the label mapping\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('label_mapping.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T17:02:42.489898Z","iopub.execute_input":"2025-01-27T17:02:42.490235Z","iopub.status.idle":"2025-01-27T17:02:53.190406Z","shell.execute_reply.started":"2025-01-27T17:02:42.490207Z","shell.execute_reply":"2025-01-27T17:02:53.188664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sklearn.metrics\n\ndef compute_eer_multiclass(y_true, y_pred_probs):\n    \"\"\"\n    Compute Equal Error Rate (EER) for a multiclass classification problem.\n    \n    :param y_true: True labels (one-hot encoded or label encoded)\n    :param y_pred_probs: Predicted probabilities from the model\n    :return: Average EER across all classes\n    \"\"\"\n    num_classes = y_pred_probs.shape[1]\n    eer_list = []\n    \n    # Compute ROC curve and EER for each class\n    for i in range(num_classes):\n        # Consider class 'i' as the positive class and all others as negative\n        true_binary = (y_true == i).astype(int)  # Convert to binary ground truth (1 for class i, 0 otherwise)\n        pred_prob = y_pred_probs[:, i]  # Predicted probabilities for class i\n        \n        # Calculate FPR, TPR, and thresholds for the ROC curve\n        fpr, tpr, thresholds = sklearn.metrics.roc_curve(true_binary, pred_prob)\n        \n        # Calculate FNR (False Negative Rate)\n        fnr = 1 - tpr\n        \n        # Find EER by finding where FPR equals FNR\n        eer_threshold = thresholds[np.nanargmin(np.absolute(fnr - fpr))]\n        \n        # Get EER value (mean of FPR and FNR)\n        eer = (fpr[np.nanargmin(np.absolute(fnr - fpr))] + fnr[np.nanargmin(np.absolute(fnr - fpr))]) / 2\n        eer_list.append(eer)\n    \n    # Return the average EER across all classes\n    average_eer = np.mean(eer_list)\n    return average_eer\n\n# Make predictions\ny_pred_probs = model_eer.predict(X_test)\n\n# Compute EER for multiclass classification\naverage_eer = compute_eer_multiclass(np.argmax(y_test, axis=1), y_pred_probs)\nprint(f\"Average Equal Error Rate (EER): {average_eer:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport csv\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Define the root directory (adjust this based on your Kaggle dataset path)\nroot_dir = \"/kaggle/input/deepfakedataset/FakeMusicCaps\"  # Adjust this path as needed\noutput_csv = \"fakemusiccaps_metadata1.csv\"\n\n# Initialize a list to store file paths and labels\ndata = []\n\n# Traverse through each subfolder in the root directory\nfor folder_name in os.listdir(root_dir):\n    folder_path = os.path.join(root_dir, folder_name)\n    \n    # Skip non-folder entries\n    if not os.path.isdir(folder_path):\n        continue\n    \n    # Iterate through each file in the folder\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        \n        # Check if the file is an audio file (adjust extensions if necessary)\n        if file_name.endswith(('.mp3', '.wav', '.flac', '.aac')):\n            data.append([file_path, folder_name])  # Add file path and folder label to data\n\n# Write the data to a CSV file\nwith open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n    writer = csv.writer(csv_file)\n    writer.writerow(['file_path', 'label'])  # Header\n    writer.writerows(data)\n\nprint(f\"Metadata CSV file has been saved as {output_csv}\")\n\n# Path to CSV file\nCSV_PATH = \"/kaggle/working/fakemusiccaps_metadata1.csv\"\n\n# Load data from CSV\ndef load_data_from_csv(csv_path):\n    data = pd.read_csv(csv_path)\n    file_paths = data['file_path'].values\n    labels = data['label'].values\n    return file_paths, labels\n\n# Preprocessing function to generate MFCC features\ndef extract_mfcc(file_path, n_mfcc=13):\n    audio, sr = librosa.load(file_path, sr=22050)  # Consistent sampling rate\n    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    return mfcc\n\n# Prepare dataset\ndef prepare_dataset(file_paths, labels, max_length=256, n_mfcc=13):\n    mfcc_features = []\n    fixed_labels = []\n    \n    for file_path, label in zip(file_paths, labels):\n        try:\n            mfcc = extract_mfcc(file_path, n_mfcc=n_mfcc)\n            if mfcc.shape[1] > max_length:\n                mfcc = mfcc[:, :max_length]\n            else:\n                padding = max_length - mfcc.shape[1]\n                mfcc = np.pad(mfcc, ((0, 0), (0, padding)), mode='constant')\n            \n            mfcc /= (np.max(np.abs(mfcc)) + 1e-8)  # Normalize after padding\n            mfcc_features.append(mfcc)\n            fixed_labels.append(label)\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    return np.array(mfcc_features), np.array(fixed_labels)\n\n# Load data\nfile_paths, labels = load_data_from_csv(CSV_PATH)\n\n# Encode labels\nunique_labels = np.unique(labels)\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\nencoded_labels = np.array([label_map[label] for label in labels])\n\n# Prepare dataset\nMAX_LENGTH = 256\nN_MFCC = 13\nmfcc_features, encoded_labels = prepare_dataset(file_paths, encoded_labels, max_length=MAX_LENGTH, n_mfcc=N_MFCC)\n\n# Add channel dimension\nmfcc_features = mfcc_features[..., np.newaxis]\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    mfcc_features, encoded_labels, test_size=0.2, random_state=42\n)\n\n# One-hot encode labels\ny_train = to_categorical(y_train, num_classes=len(unique_labels))\ny_test = to_categorical(y_test, num_classes=len(unique_labels))\n\n# Build CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(N_MFCC, MAX_LENGTH, 1)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(unique_labels), activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\n# Save the model\nmodel.save('audio_classification_cnn_mfcc2.keras')\n\n# Save the label mapping\nlabel_mapping_df = pd.DataFrame(list(label_map.items()), columns=['Label', 'Encoded Value'])\nlabel_mapping_df.to_csv('label_mapping.csv', index=False)\n\nprint(\"Model and label mapping saved successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/CodeVault-girish/MFM-models.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T15:42:32.443209Z","iopub.execute_input":"2025-03-05T15:42:32.443557Z","iopub.status.idle":"2025-03-05T15:42:33.228296Z","shell.execute_reply.started":"2025-03-05T15:42:32.443527Z","shell.execute_reply":"2025-03-05T15:42:33.227208Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'MFM-models'...\nremote: Enumerating objects: 56, done.\u001b[K\nremote: Counting objects: 100% (56/56), done.\u001b[K\nremote: Compressing objects: 100% (40/40), done.\u001b[K\nremote: Total 56 (delta 14), reused 51 (delta 11), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (56/56), 10.43 KiB | 10.43 MiB/s, done.\nResolving deltas: 100% (14/14), done.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n!ls\n\nimport sys\nsys.path.append(\"/kaggle/working/MFM-models\")  # Adjust the path if needed\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:36:39.012651Z","iopub.execute_input":"2025-03-05T16:36:39.012957Z","iopub.status.idle":"2025-03-05T16:36:39.133296Z","shell.execute_reply.started":"2025-03-05T16:36:39.012930Z","shell.execute_reply":"2025-03-05T16:36:39.131808Z"}},"outputs":[{"name":"stdout","text":"MFM-models  requirements.txt  state.db\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nprint(os.path.exists(\"/kaggle/input/deepfakedataset/FakeMusicCaps/MusicCaps\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:48:30.469718Z","iopub.execute_input":"2025-03-05T16:48:30.470184Z","iopub.status.idle":"2025-03-05T16:48:30.479181Z","shell.execute_reply.started":"2025-03-05T16:48:30.470146Z","shell.execute_reply":"2025-03-05T16:48:30.478298Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n    selection=\"1\",\n    folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/MusicCaps\",\n    output_file=\"/kaggle/working/output.csv\",  # Save output in /kaggle/working\n    device=\"cuda\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:49:06.242235Z","iopub.execute_input":"2025-03-05T16:49:06.242622Z","iopub.status.idle":"2025-03-05T16:55:54.243146Z","shell.execute_reply.started":"2025-03-05T16:49:06.242579Z","shell.execute_reply":"2025-03-05T16:55:54.242006Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Processing audio files: 100%|██████████| 5373/5373 [06:42<00:00, 13.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n selection=\"1\",\n folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/MusicGen_medium\",\n output_file=\"/kaggle/working/output2.csv\",   \n device=\"cuda\"                             \n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:56:36.946500Z","iopub.execute_input":"2025-03-05T16:56:36.946841Z","iopub.status.idle":"2025-03-05T17:04:11.212351Z","shell.execute_reply.started":"2025-03-05T16:56:36.946816Z","shell.execute_reply":"2025-03-05T17:04:11.211343Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\nWARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessing audio files: 100%|██████████| 5521/5521 [07:28<00:00, 12.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output2.csv\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n selection=\"1\",\n folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/SunoCaps\",\n output_file=\"/kaggle/working/output3.csv\",   \n device=\"cuda\"                             \n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:04:32.132654Z","iopub.execute_input":"2025-03-05T17:04:32.132995Z","iopub.status.idle":"2025-03-05T17:05:42.735662Z","shell.execute_reply.started":"2025-03-05T17:04:32.132966Z","shell.execute_reply":"2025-03-05T17:05:42.734814Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Processing audio files: 100%|██████████| 63/63 [01:09<00:00,  1.11s/it]","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output3.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n selection=\"1\",\n folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/audioldm2\",\n output_file=\"/kaggle/working/output4.csv\",   \n device=\"cuda\"                             \n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:06:15.949302Z","iopub.execute_input":"2025-03-05T17:06:15.949627Z","iopub.status.idle":"2025-03-05T17:13:49.372365Z","shell.execute_reply.started":"2025-03-05T17:06:15.949602Z","shell.execute_reply":"2025-03-05T17:13:49.371570Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\nWARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessing audio files: 100%|██████████| 5521/5521 [07:27<00:00, 12.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output4.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n selection=\"1\",\n folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/musicldm\",\n output_file=\"/kaggle/working/output5.csv\",   \n device=\"cuda\"                             \n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:14:03.045121Z","iopub.execute_input":"2025-03-05T17:14:03.045442Z","iopub.status.idle":"2025-03-05T17:21:35.764356Z","shell.execute_reply.started":"2025-03-05T17:14:03.045402Z","shell.execute_reply":"2025-03-05T17:21:35.763384Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\nWARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessing audio files: 100%|██████████| 5521/5521 [07:27<00:00, 12.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output5.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n selection=\"1\",\n folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/mustango\",\n output_file=\"/kaggle/working/output6.csv\",   \n device=\"cuda\"                             \n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:22:23.744909Z","iopub.execute_input":"2025-03-05T17:22:23.745323Z","iopub.status.idle":"2025-03-05T17:30:02.763782Z","shell.execute_reply.started":"2025-03-05T17:22:23.745287Z","shell.execute_reply":"2025-03-05T17:30:02.762830Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\nWARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessing audio files: 100%|██████████| 5521/5521 [07:33<00:00, 12.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output6.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from MFM_extractor import model_list, extract_from\nmodel_list()\nextract_from(\n selection=\"1\",\n folder_path=\"/kaggle/input/deepfakedataset/FakeMusicCaps/stable_audio_open\",\n output_file=\"/kaggle/working/output7.csv\",   \n device=\"cuda\"                             \n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:30:24.446455Z","iopub.execute_input":"2025-03-05T17:30:24.446759Z","iopub.status.idle":"2025-03-05T17:37:59.560573Z","shell.execute_reply.started":"2025-03-05T17:30:24.446736Z","shell.execute_reply":"2025-03-05T17:37:59.559401Z"}},"outputs":[{"name":"stdout","text":"Available models:\n1. MERT-v0\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at m-a-p/MERT-v0 were not used when initializing MERTModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing MERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing MERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of MERTModel were not initialized from the model checkpoint at m-a-p/MERT-v0 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"WARNING: feature_extractor_cqt requires the libray 'nnAudio'\n","output_type":"stream"},{"name":"stderr","text":"Processing audio files: 100%|██████████| 5521/5521 [07:29<00:00, 12.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved all features to /kaggle/working/output7.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"CLAP","metadata":{}},{"cell_type":"code","source":"pip install laion-clap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:30:23.053513Z","iopub.execute_input":"2025-03-09T07:30:23.053695Z","iopub.status.idle":"2025-03-09T07:30:36.712014Z","shell.execute_reply.started":"2025-03-09T07:30:23.053676Z","shell.execute_reply":"2025-03-09T07:30:36.711040Z"}},"outputs":[{"name":"stdout","text":"Collecting laion-clap\n  Downloading laion_clap-1.1.6-py3-none-any.whl.metadata (26 kB)\nCollecting numpy==1.23.5 (from laion-clap)\n  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from laion-clap) (0.12.1)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from laion-clap) (0.10.2.post1)\nCollecting torchlibrosa (from laion-clap)\n  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\nCollecting ftfy (from laion-clap)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting braceexpand (from laion-clap)\n  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting webdataset (from laion-clap)\n  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\nCollecting wget (from laion-clap)\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from laion-clap) (0.19.1)\nRequirement already satisfied: llvmlite in /usr/local/lib/python3.10/dist-packages (from laion-clap) (0.43.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from laion-clap) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from laion-clap) (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from laion-clap) (2.1.4)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from laion-clap) (3.11.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from laion-clap) (4.66.5)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from laion-clap) (2024.9.11)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from laion-clap) (4.44.2)\nCollecting progressbar (from laion-clap)\n  Downloading progressbar-2.5.tar.gz (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->laion-clap) (0.2.13)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (3.0.1)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (0.60.0)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (0.5.0.post1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (4.12.2)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->laion-clap) (1.0.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->laion-clap) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->laion-clap) (1.17.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->laion-clap) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->laion-clap) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->laion-clap) (2024.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (0.24.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->laion-clap) (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (2.9.2)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->laion-clap) (71.0.4)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->laion-clap) (2.22)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->laion-clap) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->laion-clap) (4.0.11)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers->laion-clap) (2024.6.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->laion-clap) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->laion-clap) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->laion-clap) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->laion-clap) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->laion-clap) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->laion-clap) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->laion-clap) (5.0.1)\nDownloading laion_clap-1.1.6-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\nDownloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: progressbar, wget\n  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12066 sha256=a0552a5ff6a4680bce7257f433f9dd3d0932ece763dcdb6d3df41e8bc0a89928\n  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=59a5ed996d6a60c06c9b425030d8adea8b915bed1803fecc66e020b527645adc\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built progressbar wget\nInstalling collected packages: wget, progressbar, braceexpand, numpy, ftfy, webdataset, torchlibrosa, laion-clap\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbucore 0.0.16 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nalbumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.17.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nchex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\npandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.23.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nxarray 2024.9.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed braceexpand-0.1.7 ftfy-6.3.1 laion-clap-1.1.6 numpy-1.23.5 progressbar-2.5 torchlibrosa-0.1.0 webdataset-0.2.111 wget-3.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/LAION-AI/CLAP.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:34:27.091951Z","iopub.execute_input":"2025-03-09T07:34:27.092253Z","iopub.status.idle":"2025-03-09T07:34:27.212196Z","shell.execute_reply.started":"2025-03-09T07:34:27.092229Z","shell.execute_reply":"2025-03-09T07:34:27.211327Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'CLAP' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install ffmpeg-python\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:34:57.984691Z","iopub.execute_input":"2025-03-09T07:34:57.985032Z","iopub.status.idle":"2025-03-09T07:35:01.318753Z","shell.execute_reply.started":"2025-03-09T07:34:57.985002Z","shell.execute_reply":"2025-03-09T07:35:01.317928Z"}},"outputs":[{"name":"stdout","text":"Collecting ffmpeg-python\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\nDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nInstalling collected packages: ffmpeg-python\nSuccessfully installed ffmpeg-python-0.2.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%cd CLAP\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:35:14.753831Z","iopub.execute_input":"2025-03-09T07:35:14.754217Z","iopub.status.idle":"2025-03-09T07:35:14.762108Z","shell.execute_reply.started":"2025-03-09T07:35:14.754185Z","shell.execute_reply":"2025-03-09T07:35:14.761224Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/CLAP\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:36:46.105666Z","iopub.execute_input":"2025-03-09T07:36:46.106029Z","iopub.status.idle":"2025-03-09T07:36:49.387717Z","shell.execute_reply.started":"2025-03-09T07:36:46.105991Z","shell.execute_reply":"2025-03-09T07:36:49.386727Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"your_huggingface_token_here\")  # Replace with your actual token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:37:05.745378Z","iopub.execute_input":"2025-03-09T07:37:05.745666Z","iopub.status.idle":"2025-03-09T07:37:06.425104Z","shell.execute_reply.started":"2025-03-09T07:37:05.745642Z","shell.execute_reply":"2025-03-09T07:37:06.423873Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c2511223470b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your_huggingface_token_here\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your actual token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;34m\"you want to set the git credential as well.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             )\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_permission\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_permission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mnotebook_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_permission\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_permission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(token, add_to_git_credential, write_permission)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mpermission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_permission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpermission\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid token passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mwrite_permission\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpermission\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         raise ValueError(\n","\u001b[0;31mValueError\u001b[0m: Invalid token passed!"],"ename":"ValueError","evalue":"Invalid token passed!","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"HUBERT","metadata":{}},{"cell_type":"code","source":"pip install pip==23.3.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:48:39.693843Z","iopub.execute_input":"2025-03-09T07:48:39.694225Z","iopub.status.idle":"2025-03-09T07:48:44.956776Z","shell.execute_reply.started":"2025-03-09T07:48:39.694195Z","shell.execute_reply":"2025-03-09T07:48:44.955915Z"}},"outputs":[{"name":"stdout","text":"Collecting pip==23.3.1\n  Downloading pip-23.3.1-py3-none-any.whl.metadata (3.5 kB)\nDownloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-23.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"pip install fairseq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:48:53.143052Z","iopub.execute_input":"2025-03-09T07:48:53.143392Z","iopub.status.idle":"2025-03-09T07:49:48.114486Z","shell.execute_reply.started":"2025-03-09T07:48:53.143361Z","shell.execute_reply":"2025-03-09T07:49:48.113561Z"}},"outputs":[{"name":"stdout","text":"Collecting fairseq\n  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\nCollecting hydra-core<1.1,>=1.0.7 (from fairseq)\n  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\nCollecting omegaconf<2.1 (from fairseq)\n  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.9.11)\nCollecting sacrebleu>=1.4.12 (from fairseq)\n  Using cached sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.5)\nCollecting bitarray (from fairseq)\n  Using cached bitarray-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\nRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\nCollecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\nCollecting portalocker (from sacrebleu>=1.4.12->fairseq)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2024.6.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\nDownloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitarray-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: fairseq, antlr4-python3-runtime\n  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288664 sha256=2bfdef8191b6642e69ee6383c3278f39ed3c39f33a65e38e3215fc1ba86a29b8\n  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141213 sha256=45515f6a24cabf863932d7287d95d339d523babb977730cb4a8d5e94c8a22019\n  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\nSuccessfully built fairseq antlr4-python3-runtime\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, sacrebleu, hydra-core, fairseq\n  Attempting uninstall: antlr4-python3-runtime\n    Found existing installation: antlr4-python3-runtime 4.9.3\n    Uninstalling antlr4-python3-runtime-4.9.3:\n      Successfully uninstalled antlr4-python3-runtime-4.9.3\n  Attempting uninstall: omegaconf\n    Found existing installation: omegaconf 2.3.0\n    Uninstalling omegaconf-2.3.0:\n      Successfully uninstalled omegaconf-2.3.0\nSuccessfully installed antlr4-python3-runtime-4.8 bitarray-3.1.1 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-3.1.1 sacrebleu-2.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!git clone https://github.com/bshall/hubert.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:49:56.313857Z","iopub.execute_input":"2025-03-09T07:49:56.314241Z","iopub.status.idle":"2025-03-09T07:49:56.796209Z","shell.execute_reply.started":"2025-03-09T07:49:56.314211Z","shell.execute_reply":"2025-03-09T07:49:56.795284Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'hubert'...\nremote: Enumerating objects: 141, done.\u001b[K\nremote: Counting objects: 100% (63/63), done.\u001b[K\nremote: Compressing objects: 100% (18/18), done.\u001b[K\nremote: Total 141 (delta 52), reused 48 (delta 45), pack-reused 78 (from 1)\u001b[K\nReceiving objects: 100% (141/141), 473.68 KiB | 5.51 MiB/s, done.\nResolving deltas: 100% (77/77), done.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"%cd hubert\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:50:05.461492Z","iopub.execute_input":"2025-03-09T07:50:05.461826Z","iopub.status.idle":"2025-03-09T07:50:05.467773Z","shell.execute_reply.started":"2025-03-09T07:50:05.461794Z","shell.execute_reply":"2025-03-09T07:50:05.467052Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/CLAP/hubert\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:54:15.647148Z","iopub.execute_input":"2025-03-09T07:54:15.647453Z","iopub.status.idle":"2025-03-09T07:54:24.219877Z","shell.execute_reply.started":"2025-03-09T07:54:15.647430Z","shell.execute_reply":"2025-03-09T07:54:24.218824Z"}},"outputs":[{"name":"stdout","text":"--2025-03-09 07:54:15--  https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.230.202.3, 54.230.202.7, 54.230.202.65, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.230.202.3|:443... connected.\nHTTP request sent, awaiting response... ","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"200 OK\nLength: 1136468879 (1.1G) [application/zip]\nSaving to: ‘hubert_base_ls960.pt’\n\nhubert_base_ls960.p 100%[===================>]   1.06G   137MB/s    in 8.2s    \n\n2025-03-09 07:54:24 (132 MB/s) - ‘hubert_base_ls960.pt’ saved [1136468879/1136468879]\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!pip install fairseq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:55:43.899499Z","iopub.execute_input":"2025-03-09T07:55:43.899791Z","iopub.status.idle":"2025-03-09T07:55:51.671724Z","shell.execute_reply.started":"2025-03-09T07:55:43.899769Z","shell.execute_reply":"2025-03-09T07:55:51.670787Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\nRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.17.1)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.11)\nRequirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\nRequirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.9.11)\nRequirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.5.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.5)\nRequirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.1.1)\nRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.4.1+cu121)\nRequirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\nRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.2)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (3.1.1)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2024.6.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!wget -O hubert_base_ls960.pt https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:55:56.508830Z","iopub.execute_input":"2025-03-09T07:55:56.509169Z","iopub.status.idle":"2025-03-09T07:56:09.321581Z","shell.execute_reply.started":"2025-03-09T07:55:56.509142Z","shell.execute_reply":"2025-03-09T07:56:09.320576Z"}},"outputs":[{"name":"stdout","text":"--2025-03-09 07:55:56--  https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.230.202.65, 54.230.202.3, 54.230.202.7, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.230.202.65|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1136468879 (1.1G) [application/zip]\nSaving to: ‘hubert_base_ls960.pt’\n\nhubert_base_ls960.p 100%[===================>]   1.06G  89.9MB/s    in 12s     \n\n2025-03-09 07:56:09 (89.0 MB/s) - ‘hubert_base_ls960.pt’ saved [1136468879/1136468879]\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nimport fairseq\n\n# Load the pretrained model\nmodels, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([\"hubert_base_ls960.pt\"])\n\n# Extract the model\nhubert = models[0]\nhubert.eval()  # Set to evaluation mode\n\nprint(\"HuBERT model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:56:14.598283Z","iopub.execute_input":"2025-03-09T07:56:14.598583Z","iopub.status.idle":"2025-03-09T07:56:19.811119Z","shell.execute_reply.started":"2025-03-09T07:56:14.598559Z","shell.execute_reply":"2025-03-09T07:56:19.810163Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(f, map_location=torch.device(\"cpu\"))\n","output_type":"stream"},{"name":"stdout","text":"HuBERT model loaded successfully!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport torchaudio\n\n# Function to extract embeddings\n# Function to extract embeddings\ndef extract_embeddings(audio_path):\n    waveform, sample_rate = torchaudio.load(audio_path)  # Load audio\n    waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono if stereo\n    if sample_rate != 16000:  # HuBERT requires 16kHz\n        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n\n    # Move waveform to the same device as HuBERT\n    device = next(hubert.parameters()).device\n    waveform = waveform.to(device)\n\n    with torch.no_grad():\n        features = hubert.extract_features(waveform)[0]  # Extract embeddings\n    return features.squeeze(0)  # Remove batch dimension\n\n\n# Paths\ndataset_path = \"/kaggle/input/deepfakedataset/FakeMusicCaps\"  # Root dataset folder\nsave_path = \"/kaggle/working/hubert_npy\"  # Where to save .npy files\nos.makedirs(save_path, exist_ok=True)\n\n# Process each folder\nfor folder in os.listdir(dataset_path):\n    folder_path = os.path.join(dataset_path, folder)\n\n    if os.path.isdir(folder_path):\n        print(f\"Processing folder: {folder}\")\n\n        folder_embeddings = []  # Store all embeddings for the folder\n\n        for file in os.listdir(folder_path):\n            if file.endswith(\".wav\"):\n                file_path = os.path.join(folder_path, file)\n                \n                # Extract embeddings\n                embeddings = extract_embeddings(file_path)\n                folder_embeddings.append(embeddings.cpu().numpy())  # Convert to NumPy and store\n\n        # Save all embeddings of this folder as a single .npy file\n        save_file = os.path.join(save_path, f\"{folder}.npy\")\n        np.save(save_file, np.array(folder_embeddings, dtype=object))\n\n\n        print(f\"Saved embeddings for {folder} to {save_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T08:35:42.693062Z","iopub.execute_input":"2025-03-09T08:35:42.693353Z","iopub.status.idle":"2025-03-09T08:35:47.469302Z","shell.execute_reply.started":"2025-03-09T08:35:42.693330Z","shell.execute_reply":"2025-03-09T08:35:47.468065Z"}},"outputs":[{"name":"stdout","text":"Processing folder: stable_audio_open\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f6a12ed696ad>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Extract embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mfolder_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to NumPy and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-f6a12ed696ad>\u001b[0m in \u001b[0;36mextract_embeddings\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Move waveform to the same device as HuBERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'hubert' is not defined"],"ename":"NameError","evalue":"name 'hubert' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define file paths and labels\nfile_paths = {\n    \"MusicCaps.csv\": 1,  # Label 1 for MusicCaps.csv\n    \"MusicGen_medium.csv\": 0,\n    \"SunoCaps.csv\": 0,\n    \"audioldm2.csv\": 0,\n    \"musicldm.csv\": 0,\n    \"mustango.csv\": 0,\n    \"stable_audio_open.csv\": 0\n}\n\n# Input folder (where CSVs are stored)\ninput_folder = \"/kaggle/input/output\"  # Change this based on your dataset folder\n\n# Output folder (where modified files will be saved)\noutput_folder = \"/kaggle/working/\"  # This is writable in Kaggle\n\n# Process each file\nfor file_name, label in file_paths.items():\n    input_path = os.path.join(input_folder, file_name)\n    output_path = os.path.join(output_folder, file_name)  # Save to working directory\n\n    # Check if file exists\n    if os.path.exists(input_path):\n        # Read CSV\n        df = pd.read_csv(input_path)\n\n        # Insert the label column as the first column\n        df.insert(0, \"label\", label)\n\n        # Save the modified file to Kaggle's working directory\n        df.to_csv(output_path, index=False)\n\n        print(f\"✅ Processed {file_name} with label {label} and saved to {output_path}\")\n    else:\n        print(f\"⚠️ File not found: {input_path}\")\n\nprint(\"🎯 Labeling complete! Check /kaggle/working/ for updated files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T15:31:15.653626Z","iopub.execute_input":"2025-03-11T15:31:15.653906Z","iopub.status.idle":"2025-03-11T15:31:43.287143Z","shell.execute_reply.started":"2025-03-11T15:31:15.653878Z","shell.execute_reply":"2025-03-11T15:31:43.286467Z"}},"outputs":[{"name":"stdout","text":"✅ Processed MusicCaps.csv with label 1 and saved to /kaggle/working/MusicCaps.csv\n✅ Processed MusicGen_medium.csv with label 0 and saved to /kaggle/working/MusicGen_medium.csv\n✅ Processed SunoCaps.csv with label 0 and saved to /kaggle/working/SunoCaps.csv\n✅ Processed audioldm2.csv with label 0 and saved to /kaggle/working/audioldm2.csv\n✅ Processed musicldm.csv with label 0 and saved to /kaggle/working/musicldm.csv\n✅ Processed mustango.csv with label 0 and saved to /kaggle/working/mustango.csv\n✅ Processed stable_audio_open.csv with label 0 and saved to /kaggle/working/stable_audio_open.csv\n🎯 Labeling complete! Check /kaggle/working/ for updated files.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}